{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Learn the Basics](intro.html) \\|\\|\n",
    "[Quickstart](quickstart_tutorial.html) \\|\\|\n",
    "[Tensors](01_tensorqs_tutorial.ipynb) \\|\\| [Datasets &\n",
    "DataLoaders](data_tutorial.html) \\|\\|\n",
    "[Transforms](transforms_tutorial.html) \\|\\| [Build\n",
    "Model](buildmodel_tutorial.html) \\|\\|\n",
    "[Autograd](autogradqs_tutorial.html) \\|\\| **Optimization** \\|\\| [Save &\n",
    "Load Model](saveloadrun_tutorial.html)\n",
    "\n",
    "Optimizing Model Parameters\n",
    "===========================\n",
    "\n",
    "Now that we have a model and data it\\'s time to train, validate and test\n",
    "our model by optimizing its parameters on our data. Training a model is\n",
    "an iterative process; in each iteration the model makes a guess about\n",
    "the output, calculates the error in its guess (*loss*), collects the\n",
    "derivatives of the error with respect to its parameters (as we saw in\n",
    "the [previous section](autograd_tutorial.html)), and **optimizes** these\n",
    "parameters using gradient descent. For a more detailed walkthrough of\n",
    "this process, check out this video on [backpropagation from\n",
    "3Blue1Brown](https://www.youtube.com/watch?v=tIeHLnjs5U8).\n",
    "\n",
    "Prerequisite Code\n",
    "-----------------\n",
    "\n",
    "We load the code from the previous sections on [Datasets &\n",
    "DataLoaders](data_tutorial.html) and [Build\n",
    "Model](buildmodel_tutorial.html).\n",
    "\n",
    "---\n",
    "\n",
    "**모델 매개변수 최적화**  \n",
    "===========================  \n",
    "\n",
    "이제 모델과 데이터가 준비되었으므로, 데이터를 기반으로 모델을 훈련하고 검증하며 테스트하여 매개변수를 최적화할 차례입니다. 모델 훈련은 반복적인 과정입니다. 각 반복에서 모델은 출력에 대한 추측을 하고, 그 추측의 오류(*손실*)를 계산하며, 매개변수에 대한 오류의 미분(기울기)을 수집합니다(이전 섹션에서 설명한 바와 같이). 이후 **경사 하강법**(gradient descent)을 사용하여 이러한 매개변수를 최적화합니다. 이 과정에 대한 자세한 설명은 [3Blue1Brown의 역전파(backpropagation) 동영상](https://www.youtube.com/watch?v=tIeHLnjs5U8)을 참조하세요.\n",
    "\n",
    "**선행 코드**  \n",
    "-----------------  \n",
    "\n",
    "이전 섹션에서 다룬 [데이터셋 및 데이터 로더](data_tutorial.html)와 [모델 구축](buildmodel_tutorial.html) 코드를 로드합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters\n",
    "===============\n",
    "\n",
    "Hyperparameters are adjustable parameters that let you control the model\n",
    "optimization process. Different hyperparameter values can impact model\n",
    "training and convergence rates ([read\n",
    "more](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)\n",
    "about hyperparameter tuning)\n",
    "\n",
    "We define the following hyperparameters for training:\n",
    "\n",
    ":   -   **Number of Epochs** - the number times to iterate over the\n",
    "        dataset\n",
    "    -   **Batch Size** - the number of data samples propagated through\n",
    "        the network before the parameters are updated\n",
    "    -   **Learning Rate** - how much to update models parameters at each\n",
    "        batch/epoch. Smaller values yield slow learning speed, while\n",
    "        large values may result in unpredictable behavior during\n",
    "        training.\n",
    "\n",
    "---\n",
    "\n",
    "**하이퍼파라미터**  \n",
    "===============  \n",
    "\n",
    "하이퍼파라미터는 모델 최적화 과정을 제어할 수 있게 해주는 조정 가능한 매개변수입니다. 서로 다른 하이퍼파라미터 값은 모델 훈련과 수렴 속도에 영향을 미칠 수 있습니다 ([하이퍼파라미터 튜닝에 대해 더 알아보기](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)).\n",
    "\n",
    "훈련을 위한 다음 하이퍼파라미터를 정의합니다:\n",
    "\n",
    "- **에폭 수 (Number of Epochs)** - 데이터셋을 반복하는 횟수\n",
    "- **배치 크기 (Batch Size)** - 매개변수가 업데이트되기 전에 네트워크를 통해 전파되는 데이터 샘플의 수\n",
    "- **학습률 (Learning Rate)** - 각 배치/에폭마다 모델 매개변수를 얼마나 업데이트할 것인지. 작은 값은 느린 학습 속도를 초래하며, 큰 값은 훈련 중 예측할 수 없는 행동을 유발할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3 # 1 * 10^-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization Loop\n",
    "=================\n",
    "\n",
    "Once we set our hyperparameters, we can then train and optimize our\n",
    "model with an optimization loop. Each iteration of the optimization loop\n",
    "is called an **epoch**.\n",
    "\n",
    "Each epoch consists of two main parts:\n",
    "\n",
    ":   -   **The Train Loop** - iterate over the training dataset and try\n",
    "        to converge to optimal parameters.\n",
    "    -   **The Validation/Test Loop** - iterate over the test dataset to\n",
    "        check if model performance is improving.\n",
    "\n",
    "Let\\'s briefly familiarize ourselves with some of the concepts used in\n",
    "the training loop. Jump ahead to see the\n",
    "`full-impl-label`{.interpreted-text role=\"ref\"} of the optimization\n",
    "loop.\n",
    "\n",
    "Loss Function\n",
    "-------------\n",
    "\n",
    "When presented with some training data, our untrained network is likely\n",
    "not to give the correct answer. **Loss function** measures the degree of\n",
    "dissimilarity of obtained result to the target value, and it is the loss\n",
    "function that we want to minimize during training. To calculate the loss\n",
    "we make a prediction using the inputs of our given data sample and\n",
    "compare it against the true data label value.\n",
    "\n",
    "Common loss functions include\n",
    "[nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)\n",
    "(Mean Square Error) for regression tasks, and\n",
    "[nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss)\n",
    "(Negative Log Likelihood) for classification.\n",
    "[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)\n",
    "combines `nn.LogSoftmax` and `nn.NLLLoss`.\n",
    "\n",
    "We pass our model\\'s output logits to `nn.CrossEntropyLoss`, which will\n",
    "normalize the logits and compute the prediction error.\n",
    "\n",
    "---\n",
    "\n",
    "**최적화 루프**  \n",
    "=================\n",
    "\n",
    "하이퍼파라미터를 설정한 후, 최적화 루프를 통해 모델을 훈련하고 최적화할 수 있습니다. 최적화 루프의 각 반복을 **에폭**(epoch)이라고 합니다.\n",
    "\n",
    "각 에폭은 두 가지 주요 부분으로 구성됩니다:\n",
    "\n",
    "- **훈련 루프 (The Train Loop)** - 훈련 데이터셋을 반복하며 최적의 매개변수에 수렴하려고 시도합니다.\n",
    "- **검증/테스트 루프 (The Validation/Test Loop)** - 테스트 데이터셋을 반복하여 모델 성능이 향상되고 있는지 확인합니다.\n",
    "\n",
    "훈련 루프에서 사용되는 몇 가지 개념에 대해 간단히 알아보겠습니다. 최적화 루프의 `full-impl-label`{.interpreted-text role=\"ref\"}로 바로 가실 수 있습니다.\n",
    "\n",
    "**손실 함수 (Loss Function)**  \n",
    "-------------\n",
    "\n",
    "훈련 데이터를 제공받았을 때, 훈련되지 않은 네트워크는 올바른 답을 제공하지 않을 가능성이 높습니다. **손실 함수**는 얻어진 결과와 목표 값 간의 비유사성을 측정하며, 우리는 훈련 중에 이 손실 함수를 최소화하고자 합니다. 손실을 계산하기 위해 주어진 데이터 샘플의 입력을 사용하여 예측을 수행하고, 이를 실제 데이터 레이블 값과 비교합니다.\n",
    "\n",
    "일반적인 손실 함수로는 회귀 작업에 사용되는 [nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss) (평균 제곱 오차)와 분류 작업에 사용되는 [nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss) (음의 로그 우도)가 있습니다. [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)는 `nn.LogSoftmax`와 `nn.NLLLoss`를 결합한 것입니다.\n",
    "\n",
    "모델의 출력 로짓(logits)을 `nn.CrossEntropyLoss`에 전달하면, 로짓이 정규화되고 예측 오류가 계산됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer\n",
    "=========\n",
    "\n",
    "Optimization is the process of adjusting model parameters to reduce\n",
    "model error in each training step. **Optimization algorithms** define\n",
    "how this process is performed (in this example we use Stochastic\n",
    "Gradient Descent). All optimization logic is encapsulated in the\n",
    "`optimizer` object. Here, we use the SGD optimizer; additionally, there\n",
    "are many [different\n",
    "optimizers](https://pytorch.org/docs/stable/optim.html) available in\n",
    "PyTorch such as ADAM and RMSProp, that work better for different kinds\n",
    "of models and data.\n",
    "\n",
    "We initialize the optimizer by registering the model\\'s parameters that\n",
    "need to be trained, and passing in the learning rate hyperparameter.\n",
    "\n",
    "---\n",
    "\n",
    "**최적화기 (Optimizer)**  \n",
    "=========  \n",
    "\n",
    "최적화는 각 훈련 단계에서 모델 오류를 줄이기 위해 모델 매개변수를 조정하는 과정입니다. **최적화 알고리즘**은 이 과정이 어떻게 수행되는지를 정의합니다(이 예제에서는 확률적 경사 하강법(Stochastic Gradient Descent, SGD)을 사용합니다). 모든 최적화 로직은 `optimizer` 객체에 캡슐화되어 있습니다. 여기서는 SGD 최적화를 사용하며, PyTorch에는 ADAM, RMSProp 등과 같이 다양한 [최적화기](https://pytorch.org/docs/stable/optim.html)가 있어, 모델과 데이터 유형에 따라 더 잘 작동하는 경우가 많습니다.\n",
    "\n",
    "최적화기는 훈련해야 할 모델의 매개변수를 등록하고, 학습률 하이퍼파라미터를 전달하여 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the training loop, optimization happens in three steps:\n",
    "\n",
    ":   -   Call `optimizer.zero_grad()` to reset the gradients of model\n",
    "        parameters. Gradients by default add up; to prevent\n",
    "        double-counting, we explicitly zero them at each iteration.\n",
    "    -   Backpropagate the prediction loss with a call to\n",
    "        `loss.backward()`. PyTorch deposits the gradients of the loss\n",
    "        w.r.t. each parameter.\n",
    "    -   Once we have our gradients, we call `optimizer.step()` to adjust\n",
    "        the parameters by the gradients collected in the backward pass.\n",
    "\n",
    "---\n",
    "\n",
    "훈련 루프 내에서 최적화는 세 가지 단계로 진행됩니다:\n",
    "\n",
    "- **`optimizer.zero_grad()` 호출:** 모델 매개변수의 기울기를 초기화합니다. 기울기는 기본적으로 누적되기 때문에, 이중 계산을 방지하기 위해 각 반복에서 명시적으로 0으로 설정합니다.\n",
    "  \n",
    "- **손실 역전파:** `loss.backward()`를 호출하여 예측 손실을 역전파합니다. PyTorch는 각 매개변수에 대한 손실의 기울기를 저장합니다.\n",
    "\n",
    "- **매개변수 조정:** 기울기를 확보한 후, `optimizer.step()`을 호출하여 역전파에서 수집된 기울기에 따라 매개변수를 조정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Implementation {#full-impl-label}\n",
    "===================\n",
    "\n",
    "We define `train_loop` that loops over our optimization code, and\n",
    "`test_loop` that evaluates the model\\'s performance against our test\n",
    "data.\n",
    "\n",
    "**전체 구현** {#full-impl-label}  \n",
    "===================  \n",
    "\n",
    "`train_loop`을 정의하여 최적화 코드를 반복하고, `test_loop`를 정의하여 테스트 데이터에 대한 모델의 성능을 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    # 모델을 training node로 설정 - batch normalization과 dropout layers를 위해 중요\n",
    "    # 지금 상황에선 필수적이지 않지만 최선의 연습을 위해 추가\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader): # 데이터 로더에서 배치 단위로 반복\n",
    "        pred = model(X) # 입력 X에 대한 예측\n",
    "        loss = loss_fn(pred, y) # 예측과 실제 값 y 간의 손실 계산\n",
    "\n",
    "        # BackPropagation\n",
    "        loss.backward() # 손실에 대한 기울기 계산\n",
    "        optimizer.step() # Optimizer를 사용하여 Parameters 업데이트\n",
    "        optimizer.zero_grad() # 기울기 0으로 초기화\n",
    "\n",
    "        # 100번째 배치마다 손실 출력\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "\n",
    "    # 모델을 평가 모드로 설정 - normalization과 dropout layers를 위해 중요\n",
    "    # 지금 상황에는 필요하지 않지만 최선의 연습을 위해 추가\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # `torch.no_grad()`를 사용하여 모델을 평가하면 테스트 모드에서 기울기가 계산되지 않음\n",
    "    # 이는 `requires_grad = True`인 텐서에 대한 불 필요한 기울기 계산과 메모리 사용을 줄이는 데에도 도움이 됩니다.\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader: \n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item() # 손실을 누적\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item() # 정확도 계산\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the loss function and optimizer, and pass it to\n",
    "`train_loop` and `test_loop`. Feel free to increase the number of epochs\n",
    "to track the model\\'s improving performance.\n",
    "\n",
    "---\n",
    "\n",
    "손실 함수와 최적화기를 초기화한 후, 이를 `train_loop`와 `test_loop`에 전달합니다. 에폭 수를 늘려 모델의 성능 향상을 추적해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.300094 [   64/60000]\n",
      "loss: 2.290082 [ 6464/60000]\n",
      "loss: 2.271015 [12864/60000]\n",
      "loss: 2.266727 [19264/60000]\n",
      "loss: 2.248260 [25664/60000]\n",
      "loss: 2.224128 [32064/60000]\n",
      "loss: 2.225450 [38464/60000]\n",
      "loss: 2.190200 [44864/60000]\n",
      "loss: 2.180830 [51264/60000]\n",
      "loss: 2.167274 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 2.146781 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.150030 [   64/60000]\n",
      "loss: 2.141581 [ 6464/60000]\n",
      "loss: 2.078056 [12864/60000]\n",
      "loss: 2.100466 [19264/60000]\n",
      "loss: 2.047601 [25664/60000]\n",
      "loss: 1.989044 [32064/60000]\n",
      "loss: 2.014000 [38464/60000]\n",
      "loss: 1.930779 [44864/60000]\n",
      "loss: 1.928427 [51264/60000]\n",
      "loss: 1.878672 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 1.859762 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.886954 [   64/60000]\n",
      "loss: 1.859519 [ 6464/60000]\n",
      "loss: 1.732371 [12864/60000]\n",
      "loss: 1.786255 [19264/60000]\n",
      "loss: 1.681132 [25664/60000]\n",
      "loss: 1.634204 [32064/60000]\n",
      "loss: 1.660297 [38464/60000]\n",
      "loss: 1.559531 [44864/60000]\n",
      "loss: 1.583371 [51264/60000]\n",
      "loss: 1.496910 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 1.502093 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.561669 [   64/60000]\n",
      "loss: 1.534217 [ 6464/60000]\n",
      "loss: 1.377671 [12864/60000]\n",
      "loss: 1.460480 [19264/60000]\n",
      "loss: 1.354208 [25664/60000]\n",
      "loss: 1.340696 [32064/60000]\n",
      "loss: 1.360325 [38464/60000]\n",
      "loss: 1.285177 [44864/60000]\n",
      "loss: 1.319785 [51264/60000]\n",
      "loss: 1.228443 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 1.249326 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.317103 [   64/60000]\n",
      "loss: 1.308092 [ 6464/60000]\n",
      "loss: 1.137480 [12864/60000]\n",
      "loss: 1.246044 [19264/60000]\n",
      "loss: 1.135185 [25664/60000]\n",
      "loss: 1.146631 [32064/60000]\n",
      "loss: 1.171326 [38464/60000]\n",
      "loss: 1.109883 [44864/60000]\n",
      "loss: 1.149252 [51264/60000]\n",
      "loss: 1.066489 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.086125 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.148139 [   64/60000]\n",
      "loss: 1.161301 [ 6464/60000]\n",
      "loss: 0.972462 [12864/60000]\n",
      "loss: 1.106145 [19264/60000]\n",
      "loss: 0.992482 [25664/60000]\n",
      "loss: 1.011022 [32064/60000]\n",
      "loss: 1.051030 [38464/60000]\n",
      "loss: 0.994263 [44864/60000]\n",
      "loss: 1.034478 [51264/60000]\n",
      "loss: 0.962564 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.977864 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.027148 [   64/60000]\n",
      "loss: 1.064512 [ 6464/60000]\n",
      "loss: 0.856440 [12864/60000]\n",
      "loss: 1.011491 [19264/60000]\n",
      "loss: 0.900819 [25664/60000]\n",
      "loss: 0.914965 [32064/60000]\n",
      "loss: 0.971897 [38464/60000]\n",
      "loss: 0.918349 [44864/60000]\n",
      "loss: 0.954599 [51264/60000]\n",
      "loss: 0.893155 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.903923 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.938065 [   64/60000]\n",
      "loss: 0.997185 [ 6464/60000]\n",
      "loss: 0.773286 [12864/60000]\n",
      "loss: 0.944837 [19264/60000]\n",
      "loss: 0.840313 [25664/60000]\n",
      "loss: 0.845535 [32064/60000]\n",
      "loss: 0.917161 [38464/60000]\n",
      "loss: 0.867806 [44864/60000]\n",
      "loss: 0.897658 [51264/60000]\n",
      "loss: 0.844279 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.851291 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.870429 [   64/60000]\n",
      "loss: 0.947245 [ 6464/60000]\n",
      "loss: 0.711608 [12864/60000]\n",
      "loss: 0.896176 [19264/60000]\n",
      "loss: 0.797656 [25664/60000]\n",
      "loss: 0.794630 [32064/60000]\n",
      "loss: 0.876987 [38464/60000]\n",
      "loss: 0.832785 [44864/60000]\n",
      "loss: 0.856000 [51264/60000]\n",
      "loss: 0.808113 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.812176 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.817221 [   64/60000]\n",
      "loss: 0.907618 [ 6464/60000]\n",
      "loss: 0.664390 [12864/60000]\n",
      "loss: 0.859060 [19264/60000]\n",
      "loss: 0.765725 [25664/60000]\n",
      "loss: 0.756568 [32064/60000]\n",
      "loss: 0.845243 [38464/60000]\n",
      "loss: 0.807258 [44864/60000]\n",
      "loss: 0.824303 [51264/60000]\n",
      "loss: 0.779758 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.781489 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Reading\n",
    "===============\n",
    "\n",
    "-   [Loss\n",
    "    Functions](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "-   [torch.optim](https://pytorch.org/docs/stable/optim.html)\n",
    "-   [Warmstart Training a\n",
    "    Model](https://pytorch.org/tutorials/recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
