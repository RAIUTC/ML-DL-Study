{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Learn the Basics](intro.html) \\|\\|\n",
    "[Quickstart](quickstart_tutorial.html) \\|\\| **Tensors** \\|\\| [Datasets &\n",
    "DataLoaders](data_tutorial.html) \\|\\|\n",
    "[Transforms](transforms_tutorial.html) \\|\\| [Build\n",
    "Model](buildmodel_tutorial.html) \\|\\|\n",
    "[Autograd](autogradqs_tutorial.html) \\|\\|\n",
    "[Optimization](optimization_tutorial.html) \\|\\| [Save & Load\n",
    "Model](saveloadrun_tutorial.html)\n",
    "\n",
    "Tensors\n",
    "=======\n",
    "\n",
    "Tensors are a specialized data structure that are very similar to arrays\n",
    "and matrices. In PyTorch, we use tensors to encode the inputs and\n",
    "outputs of a model, as well as the model's parameters.\n",
    "\n",
    "Tensors are similar to [NumPy's](https://numpy.org/) ndarrays, except\n",
    "that tensors can run on GPUs or other hardware accelerators. In fact,\n",
    "tensors and NumPy arrays can often share the same underlying memory,\n",
    "eliminating the need to copy data (see\n",
    "`bridge-to-np-label`{.interpreted-text role=\"ref\"}). Tensors are also\n",
    "optimized for automatic differentiation (we\\'ll see more about that\n",
    "later in the [Autograd](autogradqs_tutorial.html) section). If you're\n",
    "familiar with ndarrays, you'll be right at home with the Tensor API. If\n",
    "not, follow along!\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "텐서(Tensors)\n",
    "=======\n",
    "\n",
    "텐서는 배열 및 행렬과 매우 유사한 특수한 데이터 구조입니다. PyTorch에서는 텐서를 사용하여 모델의 입력 및 출력뿐만 아니라 모델의 매개변수도 인코딩합니다.\n",
    "\n",
    "텐서는 [NumPy의](https://numpy.org/) ndarrays와 유사하지만, 텐서는 GPU 또는 기타 하드웨어 가속기에서 실행될 수 있습니다. 실제로 텐서와 NumPy 배열은 종종 동일한 기본 메모리를 공유하여 데이터 복사의 필요성을 제거합니다 (자세한 내용은 `bridge-to-np-label`{.interpreted-text role=\"ref\"}를 참조하십시오). 텐서는 또한 자동 미분을 위해 최적화되어 있습니다 (이 부분에 대해서는 [자동 미분(Autograd)](autogradqs_tutorial.html) 섹션에서 더 자세히 다룰 것입니다). ndarrays에 익숙하다면 Tensor API도 쉽게 익힐 수 있습니다. 그렇지 않더라도, 함께 따라오세요!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing a Tensor\n",
    "=====================\n",
    "\n",
    "Tensors can be initialized in various ways. Take a look at the following\n",
    "examples:\n",
    "\n",
    "**Directly from data**\n",
    "\n",
    "Tensors can be created directly from data. The data type is\n",
    "automatically inferred.\n",
    "\n",
    "---\n",
    "\n",
    "텐서 초기화\n",
    "=====================\n",
    "\n",
    "텐서는 다양한 방법으로 초기화할 수 있습니다. 다음 예제를 살펴보세요:\n",
    "\n",
    "**데이터에서 직접 생성하기**\n",
    "\n",
    "텐서는 데이터를 사용하여 직접 생성할 수 있습니다. 데이터 유형은 자동으로 추론됩니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From a NumPy array**\n",
    "\n",
    "Tensors can be created from NumPy arrays (and vice versa - see\n",
    "`bridge-to-np-label`{.interpreted-text role=\"ref\"}).\n",
    "\n",
    "---\n",
    "\n",
    "**NumPy 배열에서**\n",
    "\n",
    "텐서는 NumPy 배열에서 생성할 수 있습니다 (반대의 경우도 가능하며, 자세한 내용은 `bridge-to-np-label`{.interpreted-text role=\"ref\"}를 참조하세요).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From another tensor:**\n",
    "\n",
    "The new tensor retains the properties (shape, datatype) of the argument\n",
    "tensor, unless explicitly overridden.\n",
    "\n",
    "---\n",
    "\n",
    "**다른 텐서에서:**\n",
    "\n",
    "새 텐서는 argument 텐서의 속성(형상, 데이터 유형)을 유지합니다. 단, 명시적으로 재정의하지 않는 경우에 한합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.6047, 0.7297],\n",
      "        [0.9613, 0.7713]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data)  # x_data의 속성을 유지합니다.\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float)  # x_data의 데이터 유형을 재정의합니다.\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With random or constant values:**\n",
    "\n",
    "`shape` is a tuple of tensor dimensions. In the functions below, it\n",
    "determines the dimensionality of the output tensor.\n",
    "\n",
    "---\n",
    "\n",
    "**무작위 또는 상수 값으로:**\n",
    "\n",
    "`shape`는 텐서의 차원을 나타내는 튜플입니다. 아래 함수에서 `shape`는 출력 텐서의 차원을 결정합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.3717, 0.4529, 0.9525],\n",
      "        [0.5768, 0.2006, 0.6163]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attributes of a Tensor\n",
    "======================\n",
    "\n",
    "Tensor attributes describe their shape, datatype, and the device on\n",
    "which they are stored.\n",
    "\n",
    "---\n",
    "\n",
    "텐서의 속성\n",
    "======================\n",
    "\n",
    "텐서의 속성은 텐서의 형상, 데이터 유형 및 저장되는 장치를 설명합니다.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations on Tensors\n",
    "=====================\n",
    "\n",
    "Over 100 tensor operations, including arithmetic, linear algebra, matrix\n",
    "manipulation (transposing, indexing, slicing), sampling and more are\n",
    "comprehensively described\n",
    "[here](https://pytorch.org/docs/stable/torch.html).\n",
    "\n",
    "Each of these operations can be run on the GPU (at typically higher\n",
    "speeds than on a CPU). If you're using Colab, allocate a GPU by going to\n",
    "Runtime \\> Change runtime type \\> GPU.\n",
    "\n",
    "By default, tensors are created on the CPU. We need to explicitly move\n",
    "tensors to the GPU using `.to` method (after checking for GPU\n",
    "availability). Keep in mind that copying large tensors across devices\n",
    "can be expensive in terms of time and memory!\n",
    "\n",
    "---\n",
    "\n",
    "텐서에 대한 연산\n",
    "=====================\n",
    "\n",
    "산술, 선형 대수, 행렬 조작(전치, 인덱싱, 슬라이싱), 샘플링 등 100개 이상의 텐서 연산이 [여기](https://pytorch.org/docs/stable/torch.html)에서 포괄적으로 설명됩니다.\n",
    "\n",
    "이러한 연산은 GPU에서 실행될 수 있으며(일반적으로 CPU보다 더 높은 속도로 수행됨), Colab을 사용하는 경우 Runtime > Change runtime type > GPU로 이동하여 GPU를 할당할 수 있습니다.\n",
    "\n",
    "기본적으로 텐서는 CPU에서 생성됩니다. 우리는 GPU의 가용성을 확인한 후 `.to` 메서드를 사용하여 텐서를 명시적으로 GPU로 이동해야 합니다. 장치 간에 큰 텐서를 복사하는 것은 시간과 메모리 측면에서 비용이 많이 들 수 있다는 점을 유의하세요!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out some of the operations from the list. If you\\'re familiar with\n",
    "the NumPy API, you\\'ll find the Tensor API a breeze to use.\n",
    "\n",
    "---\n",
    "\n",
    "리스트에서 몇 가지 연산을 시도해 보세요. NumPy API에 익숙하다면 Tensor API를 사용하는 것이 매우 간편할 것입니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standard numpy-like indexing and slicing:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: tensor([1., 1., 1., 1.])\n",
      "First column: tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:, 0]}\")\n",
    "print(f\"Last column: {tensor[..., -1]}\")\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: tensor([1, 2, 3, 4])\n",
      "First column: tensor([1, 1, 1, 1])\n",
      "Last column: tensor([4, 4, 4, 4])\n",
      "tensor([[1, 0, 3, 4],\n",
      "        [1, 0, 3, 4],\n",
      "        [1, 0, 3, 4],\n",
      "        [1, 0, 3, 4]])\n"
     ]
    }
   ],
   "source": [
    "data = [[1, 2, 3, 4],\n",
    "        [1, 2, 3, 4],\n",
    "        [1, 2, 3, 4], \n",
    "        [1, 2, 3, 4]]\n",
    "\n",
    "data_tensor = torch.tensor(data)\n",
    "print(f\"First row: {data_tensor[0]}\")\n",
    "print(f\"First column: {data_tensor[:, 0]}\")\n",
    "print(f\"Last column: {data_tensor[..., -1]}\")\n",
    "data_tensor[:,1] = 0\n",
    "print(data_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joining tensors** You can use `torch.cat` to concatenate a sequence of\n",
    "tensors along a given dimension. See also\n",
    "[torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html),\n",
    "another tensor joining operator that is subtly different from\n",
    "`torch.cat`.\n",
    "\n",
    "---\n",
    "\n",
    "**텐서 결합** `torch.cat`을 사용하여 주어진 차원에 따라 텐서의 시퀀스를 연결할 수 있습니다. `torch.cat`과 미세하게 다른 또 다른 텐서 결합 연산자인 [torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html)도 참고하세요.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Arithmetic operations**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1: \n",
      "  tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "y2: \n",
      "  tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "y3: \n",
      "  tensor([[0.6846, 0.0590, 0.6795, 0.1334],\n",
      "        [0.7311, 0.7193, 0.7131, 0.4742],\n",
      "        [0.1620, 0.4593, 0.7031, 0.9309],\n",
      "        [0.0532, 0.7897, 0.8161, 0.5051]])\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "z1: \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "z2: \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "z3: \n",
      " tensor([[0.5807, 0.7786, 0.4960, 0.1793],\n",
      "        [0.9465, 0.9595, 0.0274, 0.5889],\n",
      "        [0.7406, 0.9263, 0.7011, 0.1073],\n",
      "        [0.5287, 0.0305, 0.2977, 0.0725]])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 이것은 두 텐서 간의 행렬 곱셈을 계산합니다. y1, y2, y3는 같은 값을 가집니다.\n",
    "# ``tensor.T``는 텐서의 전치를 반환합니다.\n",
    "y1 = tensor @ tensor.T\n",
    "print(\"y1: \\n \", y1)\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "print(\"y2: \\n \", y2)\n",
    "\n",
    "\n",
    "y3 = torch.rand_like(y1)\n",
    "print(\"y3: \\n \", y3)\n",
    "print(torch.matmul(tensor, tensor.T, out=y3))\n",
    "\n",
    "\n",
    "# 이것은 원소별 곱셈을 계산합니다. z1, z2, z3는 같은 값을 가집니다.\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "\n",
    "print(\"z1: \\n\", z1, \"\\nz2: \\n\", z2, \"\\nz3: \\n\", z3)\n",
    "\n",
    "print(torch.mul(tensor, tensor, out=z3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single-element tensors** If you have a one-element tensor, for example\n",
    "by aggregating all values of a tensor into one value, you can convert it\n",
    "to a Python numerical value using `item()`:\n",
    "\n",
    "---\n",
    "\n",
    "**단일 요소 텐서** 만약 하나의 요소만 있는 텐서가 있다면, 예를 들어 텐서의 모든 값을 하나의 값으로 집계한 경우, `item()`을 사용하여 이를 Python 숫자 값으로 변환할 수 있습니다:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In-place operations** Operations that store the result into the\n",
    "operand are called in-place. They are denoted by a `_` suffix. For\n",
    "example: `x.copy_(y)`, `x.t_()`, will change `x`.\n",
    "\n",
    "---\n",
    "\n",
    "**제자리 연산** 결과를 피연산자에 저장하는 연산을 제자리 연산이라고 합니다. 이들은 `_` 접미사로 표시됩니다. 예를 들어: `x.copy_(y)`, `x.t_()`는 `x`를 변경합니다.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{tensor} \\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "tensor.add(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11., 10., 11., 11.],\n",
      "        [11., 10., 11., 11.],\n",
      "        [11., 10., 11., 11.],\n",
      "        [11., 10., 11., 11.]])\n"
     ]
    }
   ],
   "source": [
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
    "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "<p>In-place operations save some memory, but can be problematic when computing derivatives because of an immediate lossof history. Hence, their use is discouraged.</p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>참고:</strong></div>\n",
    "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "<p>제자리 연산은 메모리를 절약할 수 있지만, 즉각적인 이력 손실로 인해 미분을 계산할 때 문제가 될 수 있습니다. 따라서 이러한 사용은 권장되지 않습니다.</p>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bridge with NumPy {#bridge-to-np-label}\n",
    "=================\n",
    "\n",
    "Tensors on the CPU and NumPy arrays can share their underlying memory\n",
    "locations, and changing one will change the other.\n",
    "\n",
    "---\n",
    "\n",
    "NumPy와의 연결 {#bridge-to-np-label}\n",
    "=================\n",
    "\n",
    "CPU에서의 텐서와 NumPy 배열은 기본 메모리 위치를 공유할 수 있으며, 하나를 변경하면 다른 것도 변경됩니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor to NumPy array\n",
    "=====================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A change in the tensor reflects in the NumPy array.\n",
    "\n",
    "---\n",
    "\n",
    "텐서의 변경 사항은 NumPy 배열에 반영됩니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy array to Tensor\n",
    "=====================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes in the NumPy array reflects in the tensor.\n",
    "\n",
    "---\n",
    "\n",
    "NumPy 배열의 변경 사항은 텐서에 반영됩니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
