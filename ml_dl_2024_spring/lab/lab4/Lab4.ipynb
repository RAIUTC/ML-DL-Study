{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4: Sequential Data Modeling (RNN & Transformers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/sunukkim/workspace/ml_dl/ml_dl_2024_spring/lab/lab4/Lab4.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sunukkim/workspace/ml_dl/ml_dl_2024_spring/lab/lab4/Lab4.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sunukkim/workspace/ml_dl/ml_dl_2024_spring/lab/lab4/Lab4.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/drive\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Change directory to where this file is located\n",
    "\"\"\"\n",
    "%cd '/content/drive/...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.1.0 in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (2.1.0)\n",
      "Collecting torchvision==0.16.0\n",
      "  Downloading torchvision-0.16.0-cp38-cp38-macosx_10_13_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting torchaudio==2.1.0\n",
      "  Downloading torchaudio-2.1.0-cp38-cp38-macosx_10_13_x86_64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: filelock in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from torch==2.1.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from torch==2.1.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from torch==2.1.0) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from torch==2.1.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from torch==2.1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from torch==2.1.0) (2024.6.1)\n",
      "Requirement already satisfied: numpy in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from torchvision==0.16.0) (1.24.3)\n",
      "Requirement already satisfied: requests in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from torchvision==0.16.0) (2.32.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from torchvision==0.16.0) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from jinja2->torch==2.1.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from requests->torchvision==0.16.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from requests->torchvision==0.16.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from requests->torchvision==0.16.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from requests->torchvision==0.16.0) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from sympy->torch==2.1.0) (1.3.0)\n",
      "Downloading torchvision-0.16.0-cp38-cp38-macosx_10_13_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading torchaudio-2.1.0-cp38-cp38-macosx_10_13_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: torchvision, torchaudio\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.17.2\n",
      "    Uninstalling torchvision-0.17.2:\n",
      "      Successfully uninstalled torchvision-0.17.2\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.2.2\n",
      "    Uninstalling torchaudio-2.2.2:\n",
      "      Successfully uninstalled torchaudio-2.2.2\n",
      "Successfully installed torchaudio-2.1.0 torchvision-0.16.0\n",
      "Requirement already satisfied: torchtext==0.16.0 in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (0.16.0)\n",
      "Requirement already satisfied: tqdm in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from torchtext==0.16.0) (4.66.5)\n",
      "Requirement already satisfied: requests in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from torchtext==0.16.0) (2.32.2)\n",
      "Requirement already satisfied: torch==2.1.0 in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from torchtext==0.16.0) (2.1.0)\n",
      "Requirement already satisfied: numpy in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from torchtext==0.16.0) (1.24.3)\n",
      "Requirement already satisfied: torchdata==0.7.0 in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from torchtext==0.16.0) (0.7.0)\n",
      "Requirement already satisfied: filelock in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from torch==2.1.0->torchtext==0.16.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from torch==2.1.0->torchtext==0.16.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from torch==2.1.0->torchtext==0.16.0) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from torch==2.1.0->torchtext==0.16.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from torch==2.1.0->torchtext==0.16.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from torch==2.1.0->torchtext==0.16.0) (2024.6.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from torchdata==0.7.0->torchtext==0.16.0) (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from requests->torchtext==0.16.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from requests->torchtext==0.16.0) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from requests->torchtext==0.16.0) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from jinja2->torch==2.1.0->torchtext==0.16.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages (from sympy->torch==2.1.0->torchtext==0.16.0) (1.3.0)\n",
      "zsh:1: 2.0.0 not found\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0\n",
    "!pip install torchtext==0.16.0\n",
    "!pip install portalocker>=2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "import torchtext\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# from data.data import prepareData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pytorch version: 2.1.0, Device: mps\n",
      "Using torchtext version: 0.16.0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Using Pytorch version: {}, Device: {}\".format(torch.__version__, DEVICE))\n",
    "print(\"Using torchtext version: {}\".format(torchtext.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs for Sequential Data\n",
    "\n",
    "### AG News Dataset\n",
    "\n",
    "- News text dataset with **4 classes (news topics)**, single-labeled.\n",
    "    - Word (1), Sports (2), Business (3), Sci/Tech (4)\n",
    "- 120,000 training examples, 7,600 test examples\n",
    "- Details: <a src=\"https://pytorch.org/text/stable/datasets.html#ag-news\">https://pytorch.org/text/stable/datasets.html#ag-news</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = torchtext.datasets.AG_NEWS(root='../data')\n",
    "labels = [_, 'World', 'Sports', 'Business', 'Sci/Tech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business\n",
      "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Print the 1st element of the train data. Use the variable \"labels\" to get the label information.\n",
    "\"\"\"\n",
    "y, x = next(iter(train_data))\n",
    "print(labels[y])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sunukkim/miniconda3/envs/aiStudy/lib/python3.8/site-packages/torch/utils/data/datapipes/iter/combining.py:333: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes in train data: {1, 2, 3, 4}\n",
      "Classes in test data: {1, 2, 3, 4}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Classes in train data: {set([label for (label, text) in train_data])}\")\n",
    "print(f\"Classes in test data: {set([label for (label, text) in test_data])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Data Preprocessing\n",
    "\n",
    "- Tokenizer\n",
    "    - Splits the sentence inti lowercase **tokens**\n",
    "    - Exclude **stopwords** (if necessary)\n",
    "        - ex\\) the, of, this, oh, ...    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', ',', 'my', 'name', 'is', 'joonseok', '!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "tokenizer(\"Hi, my name is Joonseok!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'mldl1', 'class', '!']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Tokenize the sentence with the \"get_tokenizer\" function.\n",
    "\"\"\"\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "sample_sentence = \"I love MLDL1 class!\" # Modify the sample and see what the function does.\n",
    "tokenizer(sample_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vocabulary Encoder\n",
    "    - Represents a token as **integer index**.\n",
    "    - Vocabulary: tokens in train data\n",
    "    - New tokens: replace with \\<unk\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24104, 3, 1300, 951, 21, 0, 0, 764]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text) # yield: returns a generator instead of a list (faster when applying a function to a list)\n",
    "\n",
    "encoder = build_vocab_from_iterator(tokens(train_data), specials=[\"<unk>\"])\n",
    "encoder.set_default_index(encoder[\"<unk>\"])\n",
    "encoder(tokenizer(\"Hi, my name is Joonseok <unk> !\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[282, 2320, 0, 0, 2644, 764]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Encode the tokens with the \"build_vocab_from_iterator\" function.\n",
    "\n",
    "    - Reference: https://pytorch.org/text/stable/vocab.html#build-vocab-from-iterator\n",
    "\"\"\"\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "train_iterator = tokens(train_data)\n",
    "encoder = build_vocab_from_iterator(train_iterator, specials=[\"<unk>\"])\n",
    "encoder.set_default_index(encoder[\"<unk>\"])\n",
    "\n",
    "encoder(tokenizer(\"I love MLDL1 <unk> class !\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Text preprocessing pipeline\n",
    "    - Tokenizer: input sentence &rarr; tokens\n",
    "    - Encoder: tokens &rarr; integer index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: encoder(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before preprocessing\n",
      "3\n",
      "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "After preprocessing\n",
      "2\n",
      "[431, 425, 1, 1605, 14838, 113, 66, 2, 848, 13, 27, 14, 27, 15, 50725, 3, 431, 374, 16, 9, 67507, 6, 52258, 3, 42, 4009, 783, 325, 1]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get the processed data of the 1st element in train_data using text pipeline and label_pipeline.\n",
    "\"\"\"\n",
    "\n",
    "text_pipeline = lambda x: encoder(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "print(\"Before preprocessing\")\n",
    "y, x = next(iter(train_data))\n",
    "print(y)\n",
    "print(x)\n",
    "\n",
    "print(\"After preprocessing\")\n",
    "x = text_pipeline(x)\n",
    "y = label_pipeline(y)\n",
    "print(y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Data Batch Preprocessing\n",
    "\n",
    "- RNN can process <u>input with any length</u>!\n",
    "- However, to pass a **batch of inputs** to RNN, each input in the batch should have the same length to be converted as a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized collate_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\"), (3, 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.'), (3, \"Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\\\about the economy and the outlook for earnings are expected to\\\\hang over the stock market next week during the depth of the\\\\summer doldrums.\"), (3, 'Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\\\flows from the main pipeline in southern Iraq after\\\\intelligence showed a rebel militia could strike\\\\infrastructure, an oil official said on Saturday.'), (3, 'Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.'), (3, 'Stocks End Up, But Near Year Lows (Reuters) Reuters - Stocks ended slightly higher on Friday\\\\but stayed near lows for the year as oil prices surged past  #36;46\\\\a barrel, offsetting a positive outlook from computer maker\\\\Dell Inc. (DELL.O)'), (3, \"Money Funds Fell in Latest Week (AP) AP - Assets of the nation's retail money market mutual funds fell by  #36;1.17 billion in the latest week to  #36;849.98 trillion, the Investment Company Institute said Thursday.\"), (3, 'Fed minutes show dissent over inflation (USATODAY.com) USATODAY.com - Retail sales bounced back a bit in July, and new claims for jobless benefits fell last week, the government said Thursday, indicating the economy is improving from a midsummer slump.')]\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(train_data)\n",
    "sample_batch = []\n",
    "for _ in range(8):\n",
    "    sample_batch.append(next(iterator))\n",
    "\n",
    "print(sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 32\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        if processed_text.size(0) >= MAX_LEN:\n",
    "            processed_text = processed_text[:MAX_LEN]\n",
    "        else:\n",
    "            processed_text = torch.cat([processed_text, torch.zeros(MAX_LEN - processed_text.size(0))])\n",
    "        text_list.append(processed_text)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = torch.stack(text_list).long()\n",
    "    return label_list.to(device), text_list.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Batch** of encoded tokens\n",
    "    - Token length > MAX_LEN\n",
    "        - Cut the tails.\n",
    "    Token length < MAX_LEN\n",
    "        - Zero-pad.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **MAX_LEN** can be\n",
    "    - Pre-defined\n",
    "    - Minimum of each batch\n",
    "    - Maximum of each batch\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " tensor([[  431,   425,     1,  1605, 14838,   113,    66,     2,   848,    13,\n",
       "             27,    14,    27,    15, 50725,     3,   431,   374,    16,     9,\n",
       "          67507,     6, 52258,     3,    42,  4009,   783,   325,     1,     0,\n",
       "              0,     0],\n",
       "         [15874,  1072,   854,  1310,  4250,    13,    27,    14,    27,    15,\n",
       "            929,   797,   320, 15874,    98,     3, 27657,    28,     5,  4459,\n",
       "             11,   564, 52790,     8, 80617,  2125,     7,     2,   525,   241,\n",
       "              3,    28],\n",
       "         [   58,     8,   347,  4582,   151,    16,   738,    13,    27,    14,\n",
       "             27,    15,  2384,   452,    92,  2059, 27360,     2,   347,     8,\n",
       "              2,   738,    11,   271,    42,   240, 51953,    38,     2,   294,\n",
       "            126,   112],\n",
       "         [   70,  7376,    58,  1810,    29,   905,   537,  2846,    13,    27,\n",
       "             14,    27,    15,   838,    39,  4978,    58, 68871,    29,     2,\n",
       "            905,  2846,     7,   537,    70, 58874,   703,     5,   912,  2520,\n",
       "             93, 89171],\n",
       "         [   58,    92,  4379,     4,  3581,   145,     3,  7577,    23, 12282,\n",
       "              4,    36,   347,    13,   105,    14,   105,    15, 90056,    50,\n",
       "             58,    92,     3, 11312,  1732,     8, 13750,  9735,     3,  3593,\n",
       "              5,    23],\n",
       "         [  151,   152,    43,     3,    45,   355,    71,  2280,    13,    27,\n",
       "             14,    27,    15,   151,   789,  1357,   280,    10, 70411,  4433,\n",
       "            355,  2280,    11,     2,    71,    19,    58,    92,  2301,   353,\n",
       "            468, 55934],\n",
       "         [  756,  1207,   439,     7,   307,    85,    13,    31,    14,    31,\n",
       "             15,  1766,     6,     2,   407,    16,     9,   832,   756,   126,\n",
       "           2145,  1207,   439,    24,   468,   108,     1,   782,   139,     7,\n",
       "              2,   307],\n",
       "         [ 1355,  1236,   517, 13945,    38,  1416,    13,  2199,     1,   172,\n",
       "             14,  2199,     1,   172,    15,   832,   124,  5951,   113,     5,\n",
       "           2539,     7,  1232,     3,     8,    23,   571,    11,  2444,  1687,\n",
       "            439,    69]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The collate_batch function below is designed to process AG News dataset. What is the problem of this function?\n",
    "\n",
    "size가 batch는 전부 동일해야 하는데 데이터의 사이즈는 각 다르므로 equal size하게 만들어 줘야 함.\n",
    "\"\"\"\n",
    "\n",
    "###########################\n",
    "MAX_LEN = 32\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "\n",
    "        ###############################\n",
    "        # make every batch equal size #\n",
    "        ###############################\n",
    "\n",
    "        if processed_text.size(0) >= MAX_LEN:\n",
    "            processed_text = processed_text[:MAX_LEN]\n",
    "        else:\n",
    "            processed_text = torch.cat([processed_text, torch.zeros(MAX_LEN - processed_text.size(0))])\n",
    "            \n",
    "        text_list.append(processed_text)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = torch.stack(text_list).long()\n",
    "    return label_list, text_list\n",
    "\n",
    "collate_batch(sample_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, scheduler=None):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    tqdm_bar = tqdm(train_loader)\n",
    "\n",
    "    for label, text in tqdm_bar:\n",
    "        text = text.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(text)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        prediction = output.max(1, keepdim = True)(1)\n",
    "        correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "        optimizer.step()\n",
    "        tqdm_bar.set_description(\"Epoch {} - train loss: {:.6f}\".format(epoch, loss.item()))\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for label, text in tqdm(test_loader):\n",
    "            text = text.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(text)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True) [1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 32])\n",
      "torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "train_dataset = to_map_style_dataset(train_data)\n",
    "test_dataset = to_map_style_dataset(test_data)\n",
    "train_Dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "y, x = next(iter(train_Dataloader))\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 32, 64])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Use nn.Embedding() to get embedding vectors of x.\n",
    "\"\"\"\n",
    "\n",
    "vocab_size = len(encoder)\n",
    "emb_size = 64\n",
    "\n",
    "#######################\n",
    "embedding = nn.Embedding(vocab_size, emb_size)\n",
    "embedded_x = embedding(x)\n",
    "#######################\n",
    "print(embedded_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch API: Vanilla RNN\n",
    "\n",
    "<a src='https://pytorch.org/docs/stable/generated/torch.nn.RNN.html'>'https://pytorch.org/docs/stable/generated/torch.nn.RNN.html</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "rnn = nn.RNN(10, 20, 2)\n",
    "input = torch.randn(5, 64, 10)\n",
    "h0 = torch.randn(2, 64, 20)\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implement RNN layer using given variables.\n",
    "\"\"\"\n",
    "\n",
    "hidden_dim = 64\n",
    "num_layers = 1\n",
    "\n",
    "###########################\n",
    "rnn = nn.RNN(\n",
    "    input_size=emb_size,\n",
    "    hidden_size=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    batch_first=True\n",
    ")\n",
    "###########################\n",
    "\n",
    "h_0 = torch.randn(num_layers, BATCH_SIZE, hidden_dim)\n",
    "output, h_n = rnn(embedded_x, h_0)\n",
    "print(output.shape) # -> torch.Size([BATCH_SIZE, seq_len, hidden_dim])\n",
    "print(h_n.shape) # -> torch.Size([1, BATCH_SIZE, hidden_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification using RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden, embed, num_class, batch_size):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed)\n",
    "        self.rnn = nn.RNN(input_size = embed, hidden_size=hidden, num_layers=1, nonlinearity='tanh', bias=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, h = self.rnn(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch LSTM\n",
    "\n",
    "- `torch.nn.Embedding, torch.nn.LSTM`[[link]](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
    "- For simplicity, we'll use single layer LSTM for encoder & decoder.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./img/lstm.png\">\n",
    "</p>\n",
    "\n",
    "- input_size = length of x_t\n",
    "- hidden_size = dim of h_t\n",
    "- Check out `__init__`\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./img/lstm_2.png\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./img/lstm_input.png\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./img/lstm_output.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For simplicity, we'll use single layer LSTM for encoder & decoder.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./img/lstm_3.png\">\n",
    "</p>\n",
    "\n",
    "- c_t-1.shape = c_t.shape\n",
    "- h_t-1.shape = h_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedded_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/sunukkim/workspace/ml_dl/ml_dl_2024_spring/lab/lab4/Lab4.ipynb Cell 19\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sunukkim/workspace/ml_dl/ml_dl_2024_spring/lab/lab4/Lab4.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m hid_dim \u001b[39m=\u001b[39m \u001b[39m256\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sunukkim/workspace/ml_dl/ml_dl_2024_spring/lab/lab4/Lab4.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m max_length \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sunukkim/workspace/ml_dl/ml_dl_2024_spring/lab/lab4/Lab4.ipynb#X40sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(embedded_x\u001b[39m.\u001b[39mshape) \u001b[39m#-> torch.Size([64, 10, 512])\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sunukkim/workspace/ml_dl/ml_dl_2024_spring/lab/lab4/Lab4.ipynb#X40sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(hidden_0\u001b[39m.\u001b[39mshape) \u001b[39m#-> torch.Size([1, 64, 256])\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sunukkim/workspace/ml_dl/ml_dl_2024_spring/lab/lab4/Lab4.ipynb#X40sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(cell_0\u001b[39m.\u001b[39mshape) \u001b[39m#-> torch.Size([1, 64, 256])\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embedded_x' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "emb_dim = 512\n",
    "hid_dim = 256\n",
    "max_length = 10\n",
    "\n",
    "print(embedded_x.shape) #-> torch.Size([64, 10, 512])\n",
    "print(hidden_0.shape) #-> torch.Size([1, 64, 256])\n",
    "print(cell_0.shape) #-> torch.Size([1, 64, 256])\n",
    "\n",
    "lstm = nn.LSTM(input_size=emb_dim, hidden_size=hid_dim, batch_first=True)\n",
    "hiddens, (hidden, cell) = lstm(embedded_X, (hidden_0, cell_0))\n",
    "\n",
    "print(hiddens.shape) # (A)\n",
    "print(hidden.shape) # (B)\n",
    "print(cell.shape) # (C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Output contains (h_0, h_1, ..., h_n) \n",
    "    - (L, N, D*Hout)when `batch_first=False`\n",
    "    - **(N, L, D*Hout) when `batch_first=True`**\n",
    "    - Containing the output features (`h_t`)from the last layser of the LSTM, for each t."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiStudy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
