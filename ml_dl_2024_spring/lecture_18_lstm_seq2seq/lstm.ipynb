{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다층 LSTM RNN을 입력 시퀀스에 적용합니다. 입력 시퀀스의 각 요소에 대해 각 계층은 다음 함수를 계산합니다:\n",
    "\n",
    "$$\n",
    "i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi})\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf})\n",
    "$$\n",
    "\n",
    "$$\n",
    "g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg})\n",
    "$$\n",
    "\n",
    "$$\n",
    "o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho})\n",
    "$$\n",
    "\n",
    "$$\n",
    "c_t = f_t \\odot c_{t-1} + i_t \\odot g_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_t = o_t \\odot \\tanh(c_t)\n",
    "$$\n",
    "\n",
    "여기서 \\( h_t \\)는 시간 \\( t \\)에서의 은닉 상태, \\( c_t \\)는 시간 \\( t \\)에서의 셀 상태, \\( x_t \\)는 시간 \\( t \\)에서의 입력, \\( h_{t-1} \\)은 시간 \\( t-1 \\)에서의 은닉 상태 또는 시간 0에서의 초기 은닉 상태, 그리고 \\( i_t \\), \\( f_t \\), \\( g_t \\), \\( o_t \\)는 각각 입력 게이트, 망각 게이트, 셀 게이트, 출력 게이트를 의미합니다. \\( \\sigma \\)는 시그모이드 함수이며, \\( \\odot \\)는 아다마르드 곱(요소별 곱)을 나타냅니다.\n",
    "\n",
    "다층 LSTM에서 \\( l \\)번째 계층(\\( l \\geq 2 \\))의 입력 \\( x_t^{(l)} \\)는 이전 계층의 은닉 상태 \\( h_t^{(l-1)} \\)에 드롭아웃 \\( \\delta_t^{(l-1)} \\)을 곱한 것입니다. 여기서 각 \\( \\delta_t^{(l-1)} \\)는 확률적으로 0이 되는 베르누이 확률 변수입니다.\n",
    "\n",
    "만약 `proj_size > 0`이 지정되면, 투영이 있는 LSTM이 사용됩니다. 이는 LSTM 셀을 다음과 같이 변경합니다. 첫째, \\( h_t \\)의 차원이 `hidden_size`에서 `proj_size`로 변경됩니다(`W_{hi}`의 차원도 이에 맞게 변경됨). 둘째, 각 계층의 출력 은닉 상태는 학습 가능한 투영 행렬에 의해 곱해집니다: \\( h_t = W_{hr} h_t \\). 이로 인해 LSTM 네트워크의 출력 모양도 달라지게 됩니다. 자세한 내용은 [논문](https://arxiv.org/abs/1402.1128)을 참조하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파라미터\n",
    "\n",
    "- **input_size**: 입력 \\( x \\)에서 기대되는 특징(feature)의 수\n",
    "- **hidden_size**: 은닉 상태 \\( h \\)에서의 특징(feature)의 수\n",
    "- **num_layers**: 순환 계층(recurrent layer)의 수. 예를 들어, `num_layers=2`로 설정하면 두 개의 LSTM을 쌓아 스택된 LSTM을 형성하며, 두 번째 LSTM은 첫 번째 LSTM의 출력을 받아 최종 결과를 계산합니다. 기본값: 1\n",
    "- **bias**: False로 설정하면, 계층은 \\( b_{ih} \\)와 \\( b_{hh} \\) 편향 가중치를 사용하지 않습니다. 기본값: True\n",
    "- **batch_first**: True로 설정하면 입력 및 출력 텐서를 (seq, batch, feature) 대신 (batch, seq, feature)로 제공합니다. 이는 은닉 상태 또는 셀 상태에는 적용되지 않습니다. 자세한 내용은 아래의 입력/출력 섹션을 참조하십시오. 기본값: False\n",
    "- **dropout**: 0이 아닌 경우, 마지막 계층을 제외한 각 LSTM 계층의 출력에 드롭아웃 층을 추가하며, 드롭아웃 확률은 `dropout`과 같습니다. 기본값: 0\n",
    "- **bidirectional**: True로 설정하면 양방향 LSTM이 됩니다. 기본값: False\n",
    "- **proj_size**: 0보다 크면, 해당 크기의 투영을 가진 LSTM을 사용합니다. 기본값: 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "rnn = nn.LSTM(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "c0 = torch.randn(2, 3, 20)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiStudy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
