{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation: Encoder-Decoder\n",
    "\n",
    "## Implementation: Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.input_size = input_size # length of one-hot input\n",
    "        self.embedding_size = embedding_size # dimensionality of an input token (word embedding)\n",
    "        self.hidden_size = hidden_size # dimensionality of hidden representation\n",
    "        self.num_layers = num_layers # Number of layers in the LSTM\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        self.LSTM = nn.LSTM(self.embedding_size, hidden_size, num_layers, dropout = p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"./img/encoder_decoder.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    # shape: [sequence length, batch size, embedding dims]\n",
    "    embedding = self.dropout(self.embedding(x))\n",
    "\n",
    "    # outputs shape: [sequence length, batch size, hidden_size]\n",
    "    # hs, cs shape: [num_layers, batch_size, hidden_size]\n",
    "    _outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n",
    "\n",
    "    return hidden_state, cell_state\n",
    "\n",
    "EncoderLSTM.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation: Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.input_size = input_size # length of one-hot input (input language vocab size)\n",
    "        self.embedding_size = embedding_size # word embedding size\n",
    "        self.hidden_size = hidden_size # dimensionality of hidden representation\n",
    "        self.num_layers = num_layers # Number of layers in the LSTM\n",
    "        self.output_size = output_size # length of one-hot output (output language vocab size)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        self.LSTM = nn.LSTM(self.embedding_size, hidden_size, num_layers, dropout=p)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        x = x.unsqueeze(0) # shape of x: [1, batch_size]\n",
    "        embedding = self.dropout(self.embedding(x)) # shape: [1. batch_size, embedding dims]\n",
    "\n",
    "        # outputs shape: [1, batch size, hidden_size]\n",
    "        # hs, cs shape: [num_layers, batch_size, hidden_size] - hs, cs from Encoder\n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(embedding, (hidden_state, cell_state))\n",
    "        predictions = self.fc(outputs) # shape: [1, batch_size, output_size]\n",
    "\n",
    "        return predictions, hidden_state, cell_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation: Seq2seq Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq(nn.Module):\n",
    "    def __init__(self, EncoderLSTM, DecoderLSTM):\n",
    "        super(Seq2seq, self).__init__()\n",
    "        self.EncoderLSTM = EncoderLSTM\n",
    "        self.DecoderLSTM = DecoderLSTM\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        batch_size = source.shape[1] # source shape: [input language seq len, num_sentences]\n",
    "        target_len = target.shape[0] # target shape: [output language seq len, num_sentences]\n",
    "        target_vocab_size = len(english.vocab)\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size)\n",
    "        hs, cs = self.EncoderLSTM(source)\n",
    "\n",
    "        x = target[0] # Trigger token <sos>; shape: [batch_size]\n",
    "\n",
    "        for i in range(1, target_len):\n",
    "            output, hs, cs = self.DecoderLSTM(x, hs, cs)\n",
    "            outputs[i] = output\n",
    "            x = output.argmax(1)\n",
    "        return outputs # shape: [output language seq len, batch_size, target_vocab_size]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiStudy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
